{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to products_urls.csv\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://ae.kickscrew.com/collections/nike\"\n",
    "CLASS_HITS_ITEM = \"ais-Hits-item\"\n",
    "CLASS_PAGINATION_LINK = \"ais-Pagination-item--lastPage\"\n",
    "csv_file_path = \"products_urls.csv\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "\n",
    "data_list = []\n",
    "\n",
    "try:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "    wait = WebDriverWait(driver, 20)  # Adjust the timeout as needed\n",
    "    driver.get(BASE_URL)\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, CLASS_PAGINATION_LINK)))\n",
    "\n",
    "\n",
    "    # Find the last page number\n",
    "    last_page_element = driver.find_element(By.CLASS_NAME, CLASS_PAGINATION_LINK)\n",
    "    last_page_url = last_page_element.find_element(By.TAG_NAME,\"a\").get_attribute(\"href\")\n",
    "    last_page = int(last_page_url.split(\"page=\")[-1])\n",
    "\n",
    "    # Generate the list of URLs\n",
    "    urls = [BASE_URL + \"?page=\" + str(i) for i in range(310, last_page + 1)]\n",
    "\n",
    "    product_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, CLASS_HITS_ITEM)))\n",
    "\n",
    "        search_results = driver.find_elements(By.CLASS_NAME,CLASS_HITS_ITEM)\n",
    "\n",
    "        product_urls.extend([result.find_element(By.TAG_NAME,\"a\").get_attribute(\"href\") for result in search_results])\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['Product URL']\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        csv_writer.writerow(fieldnames)  # Write the header\n",
    "\n",
    "        # Write each product URL as a separate row\n",
    "        for url in product_urls:\n",
    "            csv_writer.writerow([url])\n",
    "\n",
    "    print(f\"Data has been saved to {csv_file_path}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "urls_file_path = \"products_urls.csv\"\n",
    "\n",
    "products_urls = []\n",
    "\n",
    "with open(urls_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    \n",
    "    # Skip the header row if it exists\n",
    "    next(csv_reader, None)\n",
    "\n",
    "    # Read each row and append the product URL to the list\n",
    "    for row in csv_reader:\n",
    "        products_urls.append(row[0])  # Assuming the URL is in the first (0-indexed) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"scraped_data.csv\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "data_list = []\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "wait = WebDriverWait(driver, 10)  # Adjust the timeout as needed\n",
    "\n",
    "products_urls = ['https://ae.kickscrew.com/en/products/nike-air-jordan-1-mid-linen-554724-082']\n",
    "for url in products_urls[0:1]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    soup = BeautifulSoup(page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle: nike-air-jordan-1-mid-linen-554724-082\n",
      "Title: Air Jordan 1 Mid 'College Grey'  554724-082\n",
      "Body: This two-toned pair starts with a smooth white leather base that dominates the side panels and perforated toe box, while light grey leather overlays in the mudguard, eye stays and ankle collar dress the otherwise colorless upper.SKU:554724-082Release Date: 24 Nov 2021Color: College Grey/White/Light Bone\n",
      "Vendor: Air Jordan 1\n",
      "Type: Retro Basketball Shoes\n",
      "Tags: {'Model No': '554724-082', 'Release Date': '2021-11-23', 'Series': 'Air Jordan 1', 'Nickname': 'College Grey', 'Style': 'Retro', 'Season': 'All Season', 'Designer': 'Peter Moore', 'Thickness': 'Regular Thickness', 'Closure': 'Lacing', 'Sole material': 'Rubber Sole', 'Upper': 'Mid Cut', 'Toe type': 'Round Toe', 'Heel type': 'Flat heel', 'Color way': 'grey'}\n"
     ]
    }
   ],
   "source": [
    "Handle = url.split('/products/')[-1]\n",
    "\n",
    "h1_element = soup.find('h1', class_='product-area__details__title')\n",
    "Title = h1_element.get_text(strip=True)\n",
    "Body = soup.find('div', class_=\"cc-tabs__tab__panel\",id = re.compile(\"product-tab-panel3_\")).get_text(strip=True)\n",
    "Type = soup.find('div', class_=\"vendor product-detail__gap-sm\").get_text(strip=True)\n",
    "\n",
    "product_info_raw = soup.find('div', class_=\"pdp-product-info-container\").get_text().strip().split('\\n\\n')\n",
    "product_info = {}\n",
    "for item in product_info_raw:\n",
    "    key, value = item.split('\\n', 1)  # Split only at the first occurrence of newline\n",
    "    product_info[key] = value\n",
    "\n",
    "print(f\"Handle: {Handle}\")\n",
    "print(f\"Title: {Title}\")\n",
    "print(f\"Body: {Body}\")\n",
    "print(f\"Vendor: {product_info['Series']}\")\n",
    "print(f\"Type: {Type}\")\n",
    "print(f\"Tags: {product_info}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find({\"class\": \"product-detail__form\", \"data-section\": \"gender\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MENS'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the relevant div element\n",
    "pdp_tab_header = soup.find('div', class_='pdpTabHeader')\n",
    "\n",
    "# Extract the Size Type and its value\n",
    "size_type_label = pdp_tab_header.find('label').text.strip()\n",
    "size_type_value = pdp_tab_header.find('div', class_='pdpOptionValues').text.strip()\n",
    "size_type_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m products_urls:\n\u001b[1;32m     24\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mwait\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_all_elements_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLASS_HITS_ITEM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME,CLASS_HITS_ITEM)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m search_results:\n",
      "File \u001b[0;32m~/ecommerce-web-scraping/myenv/lib/python3.8/site-packages/selenium/webdriver/support/wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mas/ecommerce-web-scraping/myenv/lib/python3.8/site-packages/selenium/webdriver/support/wait.py?line=102'>103</a>\u001b[0m     \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m>\u001b[39m end_time:\n\u001b[1;32m    <a href='file:///home/mas/ecommerce-web-scraping/myenv/lib/python3.8/site-packages/selenium/webdriver/support/wait.py?line=103'>104</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/mas/ecommerce-web-scraping/myenv/lib/python3.8/site-packages/selenium/webdriver/support/wait.py?line=104'>105</a>\u001b[0m \u001b[39mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "CLASS_HITS_ITEM = \"ais-Hits-item\"\n",
    "CLASS_HIT_IMAGE = \"hit-image\"\n",
    "CLASS_PRODUCT_PRICE = \"product-price\"\n",
    "\n",
    "\n",
    "csv_file_path = \"scraped_data.csv\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "\n",
    "data_list = []\n",
    "\n",
    "try:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 10)  # Adjust the timeout as needed\n",
    "\n",
    "\n",
    "    for url in products_urls:\n",
    "        driver.get(url)\n",
    "\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, CLASS_HITS_ITEM)))\n",
    "\n",
    "        search_results = driver.find_elements(By.CLASS_NAME,CLASS_HITS_ITEM)\n",
    "\n",
    "        for result in search_results:\n",
    "            handle = result.find_element(By.TAG_NAME,\"a\").get_attribute(\"href\").split(\"/products/\")[-1]\n",
    "            title = result.find_element(By.CLASS_NAME,CLASS_HIT_IMAGE).get_attribute(\"alt\")\n",
    "            price = result.find_element(By.CLASS_NAME,CLASS_PRODUCT_PRICE).text\n",
    "            image_url = result.find_element(By.CLASS_NAME,CLASS_HIT_IMAGE).get_attribute(\"src\")\n",
    "\n",
    "            data_list.append([handle,title, price, image_url])\n",
    "    \n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['Handle', 'Title', 'Price', 'Image URL']\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows({'Handle': handle, 'Title': title, 'Price': price, 'Image URL': image_url} for handle, title, price, image_url in data_list)\n",
    "        \n",
    "    print(f\"Data has been saved to {csv_file_path}\")\n",
    "\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42f79d4b816a607076411f3afae9968fab9f0ac83a9e2dbd43b0a0dcb526ff7a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
